# Distilling the Knowledge in a Neural Network
(2015) Geoffrey Hinton / Oriol Vinyals  
https://arxiv.org/pdf/1503.02531.pdf  

## どんなもの?（アブストと結論とイントロ）
- 機械学習の精度を上げるには、多くのモデルによるアンサンブル学習がシンプルな方法だが、作業が面倒かつ計算量の膨大さがハードルになる。
- 本論文では、分類タスクにおいてアンサンブルモデルの知識を軽量モデルに圧縮する蒸留という方法を提案する。

## 先行研究と比べて何がすごい？（関連研究）
- 多くのモデルによるアンサンブル学習で獲得された知識を小さなモデルに移植できることは先行研究で示されている。
- 上記の研究を抽象化した概念が本論文。
- 子モデルはラベルなし学習でも精度が高くなる、子モデルの性能が親モデルを超えることがある。

## どうやって有効だと検証した?（実験）
- MNISTでの検証
- 正則化ありの1200ノード×2層ではテストセットに対して67の誤分類
- 正則化なしの800ノード×2層ではテストセットに対して146の誤分類だったが、温度20の蒸留を行い、74の誤分類まで減少した。
- 子モデルの学習から3を省いたケースでも推論時の3へのエラーが少なかった。

## 技術の手法や肝は？（マテリアル&メソッド）
- 精度が高い複雑なモデル（アンサンブルやシングル）を親モデルとし、デプロイ向きの軽量モデルを子モデルとする。
- 親モデルのsoftmax層の出力を温度Tで除算することで出力をsoftにし、子モデルの学習におけるsoft targetとする。
- 子モデルがsoft targetに適合するように同じ温度Tをsoftmax層に使用して学習する。
- onehotラベルをhard targetとするが、使わなくてもいいらしい。
- hard targetの学習時とモデルの推論時には温度Tを1にする。

## 議論はある？（ディスカッション）
- アンサンブル学習した複数のスペシャリストモデルの移植については言及していない。

## 次に読むべき論文は？（参考文献）
- [Distillation for Object Detection](https://papers.nips.cc/paper/6676-learning-efficient-object-detection-models-with-knowledge-distillation.pdf)
