# DATASET DISTILLATION
(2018) Tongzhou Wang / Jun-Yan Zhu  
https://arxiv.org/pdf/1811.10959.pdf  

## どんなもの?（アブストと結論とイントロ）
- モデルではなく、データセットを蒸留する（合成画像を生成する）

## 先行研究と比べて何がすごい？（関連研究）
- 一般的な学習よりはるかに効率的になる（数回のパラメータ更新で済む）
- 使うモデルは1つで済む（knowledge distillationでは親モデル・子モデルが必要）
- 合成された画像は分布に合ったリアルな画像である必要はない（dataset pruningではリアルな画像が必要）

## どうやって有効だと検証した?（実験）
- mnistでは、蒸留データ10枚（各クラス1枚）の学習で精度94%を達成
- cifar10では、蒸留データ100枚の学習で精度54%を達成

## 技術の手法や肝は？（マテリアル&メソッド）
- モデルの重みの最適化ではなく、合成画像のピクセル値を最適化する

## 議論はある？（ディスカッション）
- 固定重みでは成功、ランダム重みではfine tuningとして使える
- mnistとcifar10でしか検証していない

## 次に読むべき論文は？（参考文献）
